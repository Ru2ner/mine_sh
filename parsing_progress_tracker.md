# TOKENIZER

Breaks down the raw input of the user into tokens : pipe, redirections, files, etc...
Outputs a linked lists with all the tokens, that will be later processed by the parser.

* fetch_user_input(readline)  :x:
* trim_whitespaces  :x:
* create_tokens  :white_check_mark:
* error_handling  :x:

#### TOKENS

* word  :white_check_mark:
* cmd  :x:
* cmd_with_args  :x:
* pipe  :white_check_mark:
* redirections(< && >)  :white_check_mark:
* redirections_append_and_here_doc(<< && >>)  :white_check_mark:
* infiles_and_outfiles  :x:

# PARSER

Processes the tokens generated by the tokenizer and organizes them according to a defined order :
 - group tokens into commands with their args
 - build an AST (abstract syntax tree) for complex commands involving pipes etc...

* syntax_validation  :x:
* command_grouping  :x:
* ast_construction  :x: